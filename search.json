[
  {
    "objectID": "ui.html",
    "href": "ui.html",
    "title": "User Interfaces",
    "section": "",
    "text": "Jupyter Notebook Interface\n\n\n\n\nfrom scholaris.core import *\n\n\nassistant = Assistant()\n\nLoaded Semantic Scholar API key from the environment variables.\nLoaded email address from the environment variables.\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\n\n\n\nassistant.chat(\"Tell me about your tools\")\n\nI can assist with a variety of tasks, including:\n\n* Extracting text from PDF files\n* Retrieving titles and first authors of research articles\n* Summarizing local documents (PDF, markdown, or text)\n* Describing Python code in local files\n* Converting IDs (PMIDs, PMCIDs, DOIs) using the id_converter_tool\n* Querying OpenAlex API for article metadata based on title, PMID, PMCID, or DOI\n* Querying Semantic Scholar Academic Graph API for article metadata based on title, PMID, or DOI\n\nPlease let me know how I can assist you with any of these capabilities!\n\n\n'I can assist with a variety of tasks, including:\\n\\n* Extracting text from PDF files\\n* Retrieving titles and first authors of research articles\\n* Summarizing local documents (PDF, markdown, or text)\\n* Describing Python code in local files\\n* Converting IDs (PMIDs, PMCIDs, DOIs) using the id_converter_tool\\n* Querying OpenAlex API for article metadata based on title, PMID, PMCID, or DOI\\n* Querying Semantic Scholar Academic Graph API for article metadata based on title, PMID, or DOI\\n\\nPlease let me know how I can assist you with any of these capabilities!'\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo list the files in the local data directory that has been created at initialization, simply use the Python built-in os module. Pass the assistant’s data_dir attribute as an argument to the os.listdir() function, like so:\nimport os\nos.listdir(assistant.dir_path)\nYou can alos use Python’s built-in functions to copy files from a designated source (e.g., Downloads) to the local data directory that was created at initialization. For example, to copy a file named example.md from the Downloads directory to the local data directory, use the following code:\nimport shutil\nshutil.copy(os.path.expanduser('~/Downloads/example.md'), assistant.dir_path)\nLikewise, to remove a file from the local data directory, use the following code:\nos.remove(os.path.join(assistant.dir_path, 'example.md'))",
    "crumbs": [
      "User & Developer Guide",
      "User Interfaces"
    ]
  },
  {
    "objectID": "ui.html#working-in-a-jupyter-notebook-environment",
    "href": "ui.html#working-in-a-jupyter-notebook-environment",
    "title": "User Interfaces",
    "section": "",
    "text": "Jupyter Notebook Interface\n\n\n\n\nfrom scholaris.core import *\n\n\nassistant = Assistant()\n\nLoaded Semantic Scholar API key from the environment variables.\nLoaded email address from the environment variables.\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\n\n\n\nassistant.chat(\"Tell me about your tools\")\n\nI can assist with a variety of tasks, including:\n\n* Extracting text from PDF files\n* Retrieving titles and first authors of research articles\n* Summarizing local documents (PDF, markdown, or text)\n* Describing Python code in local files\n* Converting IDs (PMIDs, PMCIDs, DOIs) using the id_converter_tool\n* Querying OpenAlex API for article metadata based on title, PMID, PMCID, or DOI\n* Querying Semantic Scholar Academic Graph API for article metadata based on title, PMID, or DOI\n\nPlease let me know how I can assist you with any of these capabilities!\n\n\n'I can assist with a variety of tasks, including:\\n\\n* Extracting text from PDF files\\n* Retrieving titles and first authors of research articles\\n* Summarizing local documents (PDF, markdown, or text)\\n* Describing Python code in local files\\n* Converting IDs (PMIDs, PMCIDs, DOIs) using the id_converter_tool\\n* Querying OpenAlex API for article metadata based on title, PMID, PMCID, or DOI\\n* Querying Semantic Scholar Academic Graph API for article metadata based on title, PMID, or DOI\\n\\nPlease let me know how I can assist you with any of these capabilities!'\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo list the files in the local data directory that has been created at initialization, simply use the Python built-in os module. Pass the assistant’s data_dir attribute as an argument to the os.listdir() function, like so:\nimport os\nos.listdir(assistant.dir_path)\nYou can alos use Python’s built-in functions to copy files from a designated source (e.g., Downloads) to the local data directory that was created at initialization. For example, to copy a file named example.md from the Downloads directory to the local data directory, use the following code:\nimport shutil\nshutil.copy(os.path.expanduser('~/Downloads/example.md'), assistant.dir_path)\nLikewise, to remove a file from the local data directory, use the following code:\nos.remove(os.path.join(assistant.dir_path, 'example.md'))",
    "crumbs": [
      "User & Developer Guide",
      "User Interfaces"
    ]
  },
  {
    "objectID": "ui.html#use-the-assistant-with-streamlit",
    "href": "ui.html#use-the-assistant-with-streamlit",
    "title": "User Interfaces",
    "section": "Use the assistant with Streamlit",
    "text": "Use the assistant with Streamlit\n\n\n\n\nStreamlit Interface\n\n\n\nTo use the assistant with Streamlit, you need to install the Streamlit package.\n\n\n\n\n\n\nNote\n\n\n\nRemember to set up a virtual environment for your project before installing Streamlit. This will help you avoid conflicts with other packages that you may have installed in your base environment.\n\n\nYou can install Streamlit using pip:\npip install streamlit\nThen, download the ui.py file from the scholaris directory in the Scholaris repository. Run the following command in the terminal to download the file to your current working directory:\nwget https://raw.githubusercontent.com/nicomarr/scholaris/main/ui.py\nIf wget is not installed on your local computer, you may also use curl, like so:\ncurl -O https://raw.githubusercontent.com/nicomarr/scholaris/main/ui.py\nTo run the streamlit app, activate the virtual environment. Then, go to the directory where the ui.py file is located and run the following command in the terminal:\nstreamlit run ui.py\nThis will start the local server and the app will be accessible in your local network via a web browser. Alternatively, you can run the command in the terminal with an additional flag so that the app will only be accessible from your local computer:\nstreamlit run ui.py --browser.serverAddress localhost\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t want to download the ui.py file from Github, you can also copy the code block from below and paste it into a new Python file in your working directory. Then run it using the command streamlit run &lt;filename&gt;.py.\n\n\nCode\nfrom scholaris.core import *\nimport streamlit as st\nimport ollama\nfrom datetime import datetime\nimport json\nimport os\nfrom pathlib import Path\n\ndef download_conversation_history():\n    now = datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%Ss\")\n    conversation_history = st.session_state.assistant.messages\n    filename = f\"conversation_history_{now}.json\"\n    return json.dumps(conversation_history, indent=2), filename\n\ndef show_local_data(dir_path: Path) -&gt; None:\n    try:\n        for file in dir_path.iterdir():\n            st.write(f\"{file.name}\")\n    except Exception as e:\n        st.write(f\"Error: {e}\")\n    \ndef handle_file_uploads(uploaded_files):\n    dir_path = st.session_state.assistant.dir_path\n    if not dir_path.exists():\n        return\n\n    for uploaded_file in uploaded_files:\n        file_path = st.session_state.assistant.dir_path / uploaded_file.name\n        if file_path.exists():\n            continue\n        try:\n            with open(file_path, \"wb\") as f:\n                f.write(uploaded_file.getbuffer())\n            if \"uploaded_files\" not in st.session_state:\n                st.session_state[\"uploaded_files\"] = []\n            st.session_state[\"uploaded_files\"].append(file_path)\n        except Exception as e:\n            error_placeholder = st.empty()\n            error_placeholder.error(f\"Error saving {uploaded_file.name}: {str(e)}\")\n            time.sleep(1)  # Display error for 1 second\n            error_placeholder.empty()\n    uploaded_files = []\n    \ndef delete_uploaded_files():\n    if \"uploaded_files\" in st.session_state and st.session_state[\"uploaded_files\"] != []:\n        file_count = 0\n        for file_path in st.session_state[\"uploaded_files\"]:\n            try:\n                os.remove(file_path)\n                file_count += 1\n            except Exception as e:\n                error_placeholder = st.empty()\n                error_placeholder.error(f\"Error deleting {file_path}: {str(e)}\")\n                time.sleep(1)  # Display error for 1 seconds\n                error_placeholder.empty()\n        st.write(f\"{file_count} file(s) deleted.\")\n        st.session_state.uploaded_files = []\n\ndef get_last_tool_names(messages):\n    \"\"\"Returns the tool names from the most recent tool call in the messages.\"\"\"\n    tool_names = []\n    for message in reversed(messages):\n        if message[\"role\"] == \"assistant\" and \"tool_calls\" in message:\n            for fn in message[\"tool_calls\"]:\n                name = fn[\"function\"].get(\"name\")\n                if name:\n                    tool_names.append(name)\n            break # Exit the loop after the first occurrence\n    return \", \".join(tool_names) if tool_names else \"No tools used.\"\n\n# Page configuration\nst.set_page_config(\n        page_title=\"Scholaris\", \n        page_icon=\":speech_balloon:\",\n        layout=\"wide\", \n        initial_sidebar_state=\"collapsed\", \n        menu_items={\n            \"About\": \"This is a graphical user interface for Scholaris, a conversational AI assistant for academic research.\",\n            \"Get Help\": \"https://github.com/nicomarr/scholaris/blob/main/nbs/02_ui.ipynb\",\n            \"Report a Bug\": \"https://github.com/nicomarr/scholaris/issues/new\"\n        }\n        )\n\n# Initialize the assistant if not already in session state\nif \"assistant\" not in st.session_state:\n    try:\n        st.session_state.assistant = Assistant(model=\"llama3.1\", dir_path=\"./data\")\n    except Exception as e:\n        if \"[Errno 61] Connection refused\" in str(e):\n            st.error(f\"\"\"An error occurred: {e}. Please make sure Ollama is installed on your local computer and the server is running.\n            For troubleshooting, refer to the Ollama docs of GitHub:\n            [README](https://github.com/ollama/ollama/blob/main/docs/README.md)\n            [FAQ](https://github.com/ollama/ollama/blob/main/docs/faq.md).\n            \"\"\")\n        else:\n            st.error(f\"An error occurred: {e}\")\n        st.stop()\n\n# Initialize other session state variables\nif \"uploaded_files\" not in st.session_state:\n    st.session_state.uploaded_files = []\n\n# Start conversation\nif len(st.session_state.assistant.messages) &lt; 2: # &lt;2 because the assistant is initialized with a system message\n    st.session_state.assistant.messages.append({\"role\": \"assistant\", \"content\": \"How can I help you?\"})\n\nif st.session_state.assistant:\n    with st.sidebar:\n        # Set up the sidebar\n        st.title(\"Scholaris\")\n        st.caption(str(st.session_state.assistant))\n        # Conversation History Section\n        st.write(\"---\")\n        st.subheader(\"Conversation History\") \n        st.download_button(label=\"Download\", \n            help=\"Download the conversation history as a JSON file.\",\n            data=download_conversation_history()[0],\n            file_name=download_conversation_history()[1],\n            mime=\"text\")\n\n        if st.button(label=\"Reset\", type=\"primary\", \n            help=\"Click to restart the conversation.\", \n            key=\"reset_messages\"):\n                st.session_state.assistant.clear_conversation_history()\n                st.session_state.assistant.messages.append({\"role\": \"assistant\", \"content\": \"How can I help you?\"})\n                st.write(\"Conversation history cleared!\")\n            # st.rerun()  # Rerun the script to update the chat interface and the sidebar\n\n        # Local Data Section\n        st.write(\"---\")\n        st.subheader(\"Local Data\")\n        dir_path = st.session_state.assistant.dir_path\n        with st.expander(\"Data files\"): \n            show_local_data(dir_path)\n\n        # File Upload Section\n        uploaded_files = st.file_uploader(\n            label=\"Upload files\",\n            type=['pdf','txt','md','markdown','py'],\n            accept_multiple_files=True,\n            )\n        if uploaded_files:\n            handle_file_uploads(uploaded_files)\n\n        # Delete Uploaded Files Section\n        if st.session_state.uploaded_files:\n            if st.button(\n                label=\"Delete uploaded files\", \n                type=\"primary\",\n                help=\"Only uploaded files will be deleted. Already existing files will not be deleted.\", \n                key=\"delete_data_files\"\n            ):\n                delete_uploaded_files()\n\n        st.write(\"---\")\n        st.write(\"Source code available on [GitHub](https://github.com/nicomarr/scholaris/blob/main/scholaris/ui.py)\")\n\n\n    # Main Chat Interface\n    for msg in st.session_state.assistant.messages: # Display chat messages from history on app rerun\n        if msg[\"role\"] == \"system\" or msg[\"role\"] == \"tool\": # Skip system message and tool returns\n            continue\n        elif msg[\"role\"] == \"assistant\" and \"content\" not in msg: # Skip tool calls where no content is returned\n            continue\n        with st.chat_message(msg[\"role\"]):\n            st.markdown(msg[\"content\"])\n    \n    if prompt := st.chat_input(): # Await user input\n        with st.chat_message(\"user\"): # Display user message in chat message container\n            st.markdown(prompt)\n        with st.spinner(\"Thinking...\") as status: # Display status while assistant is processing\n            with st.chat_message(\"assistant\"):\n                stream = st.session_state.assistant.chat(prompt=prompt, redirect_output=True)\n                try:\n                    if stream:\n                        response = st.write_stream(stream)\n                    else:\n                        st.markdown(\"I'm sorry, I am unable to respond to that query.\")\n                except Exception as e:\n                    st.error(f\"An error occurred: {e}\")\n        with st.popover(\"Tools used\"):\n            st.markdown(get_last_tool_names(st.session_state.assistant.messages))",
    "crumbs": [
      "User & Developer Guide",
      "User Interfaces"
    ]
  },
  {
    "objectID": "core.html#helper-functions",
    "href": "core.html#helper-functions",
    "title": "Source Code",
    "section": "Helper functions",
    "text": "Helper functions\nThe Ollama framework supports tool calling (also referred to as function calling) with models such as Llama 3.1. To leverage function calling, we need to pass the JSON schema for any given function as an argument to the LLM. This is the information based on which the LLM infers the most appropriate tool to use given a prompt, and which parameters/arguments to pass to a function. To simplify the process of generating JSON schemas, use the helper and decorator functions defined below.\n\nsource\n\ngenerate_json_schema\n\n generate_json_schema (func:Callable)\n\n*Generate a JSON schema for the given function based on its annotations and docstring.\nArgs: func (Callable): The function to generate a schema for.\nReturns: Dict[str, Any]: A JSON schema for the function.*\n\nsource\n\n\njson_schema_decorator\n\n json_schema_decorator (func:~T)\n\n*Decorator to generate and attach a JSON schema to a function.\nArgs: func (Callable): The function to decorate.\nReturns: Callable: The decorated function with an attached JSON schema.*",
    "crumbs": [
      "User & Developer Guide",
      "Source Code"
    ]
  },
  {
    "objectID": "core.html#local-file-processing-listing-content-extraction-and-summarization",
    "href": "core.html#local-file-processing-listing-content-extraction-and-summarization",
    "title": "Source Code",
    "section": "Local file processing: listing, content extraction, and summarization",
    "text": "Local file processing: listing, content extraction, and summarization\n\n\n\n\n\n\nNote\n\n\n\nBelow are the core functions the assistant can call. With these functions, the assistant will be able to get a list of file names in a specific data directory, can extract content from these files, and summarize them.\n\n\n\nsource\n\nget_file_names\n\n get_file_names (ext:str='pdf, txt')\n\n*Retrieves a list of file names with specified extensions in a local data directory the assistant has access to on the user’s computer.\nArgs: ext: A comma-separated string of file extensions to filter the files by. Options are: pdf, txt, md, markdown, csv, and py. Defaults to “pdf, txt”.\nReturns: str: A comma-separated string of file names with the specified extensions. If no files are found, a message is returned.\nExample: &gt;&gt;&gt; get_file_names(ext=“pdf, txt”)\n\"List of file names with the specified extensions in the local data directory: file1.pdf, file2.txt\"*\n\n\n\n\n\n\nNote\n\n\n\nThe next two functions are executed when calling the tool get_titles_and_first_authors, which is defined below, to extract the title and first author from PDF files in the local data directory. The first function, extract_text_from_pdf, extracts text from a PDF file using the PyPDF2 library. The second function, extract_title_and_first_author, will then use the LLM to extract the title and first author from the text extracted from the PDF file.\nIn addition, the extract_text_from_pdf function can also be called directly by the assistant to extract user-specified pages or sections from a PDF file, to respond to user queries or to extract specific information from a PDF file specified by the user.\n\n\n\nsource\n\n\nextract_text_from_pdf\n\n extract_text_from_pdf (file_name:str, page_range:Optional[str]=None)\n\n*A function that extracts text from a PDF file. Use this tool to extract specific details from a PDF document, such as abstract, authors, conclusions, or user-specified content of other sections. If the user specifies a page rage, use the optional page_range parameter to extract text from specific pages. If the user uses words such as beginning, middle, or end, to descripe the section, infer the page range based on the total number of 15 pages in a document. Do not use this tool to summarize an entire PDF document. Only use this tool for documents with extensions .pdf, or .PDF.\nArgs: file_name (str): The file name of the PDF document in the local data directory. page_range (Optional[str]): A string with page numbers and/or page ranges separated by commas (e.g., “1” or “1, 2”, or “5-7”). Default is None to extract all pages.\nReturns: str: Extracted text from the PDF.\nExample: &gt;&gt;&gt; text = extract_text_from_pdf(“./test.pdf”, page_range=“1”)*\n\nsource\n\n\nextract_title_and_first_author\n\n extract_title_and_first_author (contents:List[Dict[str,str]],\n                                 model:str='llama3.1',\n                                 verbose:Optional[bool]=False,\n                                 show_progress:Optional[bool]=False)\n\n*A function that extracts the titles and the first author’s names from the text of one or more research articles.\nArgs: contents (List[Dict[str, str]]): A list of dictionaries containing the file name and extracted text. model (str): The model to use for the extraction. Default is ‘llama3.1’. verbose (Optional[bool]): Whether to print additional information. Default is False. show_progress (Optional[bool]): Whether to show a progress bar. Default is False.\nReturns: contents (List[Dict[str, str]]): The input list of dictionaries with the extracted title and first author added.\nRaises: JSONDecodeError: If the JSON response is invalid.\nExample: &gt;&gt;&gt; contents = extract_title_and_first_author(contents) Extracting titles and first authors: 100%|██████████| 3/3 [00:22&lt;00:00, 7.35s/it]*\n\n\n\n\n\n\nNote\n\n\n\nThe next function combines the two functions extract_text_from_pdf and extract_title_and_first_author to extract the title and first author from PDF files in the local data directory. This function will be callable by the LLM.\n\n\n\nsource\n\n\nget_titles_and_first_authors\n\n get_titles_and_first_authors ()\n\n*A function that retrieves the titles of research articles from a directory of PDF files.\nReturns: str: A JSON-formatted string containing the titles, first authors and file names of the research articles.\nRaises: FileNotFoundError: If the specified directory does not exist.\nExample: &gt;&gt;&gt; get_titles_and_first_authors()*\n\n\n\n\n\n\nNote\n\n\n\nThe functions below are executed to summarize the content of files in the local data directory.\n\n\n\nsource\n\n\nsummarize_local_document\n\n summarize_local_document (file_name:str, ext:str='pdf')\n\n*Summarize the content of a single PDF, markdown, or text document from the local data directory.\nArgs: file_name (str): The file name of the local document to summarize. ext (str): The extension of the local document. Options are: pdf, txt, md, and markdown. Defaults to “pdf”.\nReturns: str: The summary of the content of the local document.\nExample: &gt;&gt;&gt; summarize_local_document(“research_paper”, ext=“pdf”)*\n\nsource\n\n\ndescribe_python_code\n\n describe_python_code (file_name:str)\n\n*Describe the purpose of the Python code in a local Python file. This may involve summarizing the entire code, extracting key functions, or providing an overview of the code structure.\nArgs: file_name (str): The file name of the local Python code file document to describe.\nReturns: str: A description of the purpose of the Python code in the local file.\nExample: &gt;&gt;&gt; describe_python_code(“main.py”, ext=“py”)*",
    "crumbs": [
      "User & Developer Guide",
      "Source Code"
    ]
  },
  {
    "objectID": "core.html#external-data-retrieval-from-ncbi-openalex-and-semantic-scholar",
    "href": "core.html#external-data-retrieval-from-ncbi-openalex-and-semantic-scholar",
    "title": "Source Code",
    "section": "External data retrieval from NCBI, OpenAlex and Semantic Scholar",
    "text": "External data retrieval from NCBI, OpenAlex and Semantic Scholar\n\n\n\n\n\n\nNote\n\n\n\nThe following functions are used to convert article IDs between different formats and detect the type of an article ID based on its format.\n\nconvert_id: Converts article IDs between PubMed Central, PubMed, DOI, and manuscript ID formats using the NCBI ID Converter API.\ndetect_id_type: Analyzes a given string to determine if it’s a PMID, PMCID, DOI, OpenAlex ID, Semantic Scholar ID, potential article title, or an unknown format.\nid_converter_tool: Combines the functionality of the above two functions to process a list of IDs, detecting their types and converting them using the NCBI API. This is callable by the LLM assistant.\n\n\n\n\nsource\n\nid_converter_tool\n\n id_converter_tool (ids:List[str])\n\n*For any article(s) in PubMed Central, find all the corresponding PubMed IDs (PMIDs), digital object identifiers (DOIs), and manuscript IDs (MIDs). Use this tool to convert a list of IDs, such as PMIDs, PMCIDs, or DOIs, and find the corresponding IDs for the same articles.\nArgs: ids (str): A string with a comma-separated list of IDs to convert. Must be PMIDs, PMCIDs, or DOIs. The maximum number of IDs per request is 200.\nReturns: str: A JSON-formatted string containing the conversion results and the detected ID types.*\n\nsource\n\n\ndetect_id_type\n\n detect_id_type (id_string:str)\n\n*Detect the type of the given ID or title.\nArgs: id_string (str): The ID or title to detect.\nReturns: str: The detected type (‘pmid’, ‘pmcid’, ‘doi’, ‘openalex’, ‘semantic_scholar’, ‘potential_title’, or ‘unknown’).*\n\nsource\n\n\nconvert_id\n\n convert_id (ids:List[str])\n\n*For any article(s) in PubMed Central, find all the corresponding PubMed IDs (PMIDs), digital object identifiers (DOIs), and manuscript IDs (MIDs).\nArgs: ids (List[str]): A list of IDs to convert (max 200 per request).\nReturns: Str: A JSON-formatted string containing the conversion results.*\n\n\n\n\n\n\nNote\n\n\n\nThe function query_openalex_api below is executed to query the OpenAlex database for additional information about a given a article, either by title, PubMed ID, PMC ID, or DOI.\n\n\n\nsource\n\n\nquery_openalex_api\n\n query_openalex_api (query_param:str)\n\n*Retrieve metadata for a given article from OpenAlex, a comprehensive open-access catalog of global research papers. Use this tool to search the OpenAlex API by using the article title, the PubMed ID (PMID), the PubMed Central ID (PMCID) or the digital object identifier (DOI) of an article as the query parameter. This tool returns the following metadata: - the OpenAlex ID - the digital object identifier (DOI) URL - Citation count - The open access status - URL to the open-access location for the work - Publication year - A URL to a website listing works that have cite the article - The type of the article Use this tool only if an article title, PubMed ID or DOI is provided by the user or was extracted from a local PDF file and is present in the conversation history.\nArgs: query_param (str): The article title, the PubMed ID (PMID), the PubMed Central ID (PMCID) or the digital object identifier (DOI) of the article to retrieve metadata for. May be provided by the user or extracted from a local PDF file and present in the conversation history.\nReturns: str: A JSON-formatted string including the search results from the OpenAlex database. If no results are found or the API query fails, an appropriate message is returned.*\n\n\n\n\n\n\nNote\n\n\n\nThe function query_semantic_scholar_api() below is executed to query the Semantic Scholar database for additional information about a given article, either by title, PubMed ID, or DOI. To increase the rate limit, provide your own API key (see the documentation for more information).\n\n\n\nsource\n\n\nquery_semantic_scholar_api\n\n query_semantic_scholar_api (query_param:str)\n\n*Retrieve metadata for a given article from the Semantic Scholar Academic Graph (S2AG), a large knowledge graph of scientific literature that combines data from multiple sources. Use this tool to query the Semantic Scholar Graph API by using either the article title, the PubMed ID, or the digital object identifier (DOI) to retrieve the following metadata: - the title - the publication year - the abstract - a tldr (too long, didn’t read) summary - the authors of the article - the URL to the open-access PDF version of the article, if available - the journal name - a url to the article on the Semantic Scholar website Use this tool only if an article title, PubMed ID or DOI is provided by the user or was extracted from a local PDF file and is present in the conversation history.\nArgs: query_param (str): The article title, the PubMed ID, or the digital object identifier of the article to retrieve metadata for. May be provided by the user or extracted from a local PDF file and present in the conversation history. Do not include the ‘https://doi.org/’ prefix for DOIs, or keys such as ‘DOI’, ‘PMCID’ or ‘PMID’. The tool will automatically detect the type of identifier provided.\nReturns: str: A JSON-formatted string including the search results from the Semantic Scholar database. If no results are found or the API query fails, an appropriate message is returned.*\n\nsource\n\n\nrespond_to_generic_queries\n\n respond_to_generic_queries ()\n\n*A function to respond to generic questions or queries from the user. Use this tool if no better tool is available.\nThis tool does not take any arguments.\nReturns: str: A response to a generic question.*",
    "crumbs": [
      "User & Developer Guide",
      "Source Code"
    ]
  },
  {
    "objectID": "core.html#assistant-class",
    "href": "core.html#assistant-class",
    "title": "Source Code",
    "section": "Assistant class",
    "text": "Assistant class\n\n\n\n\n\n\nNote\n\n\n\nThe Assistant class below is defined to simplify the process of chat and tool use.\n\n\n\nsource\n\nAssistant\n\n Assistant (status:dict={}, sys_message:str=None,\n            model:str='llama3.1:latest',\n            tools:Dict[str,Any]={'get_file_names': &lt;function\n            get_file_names at 0x7f6c2f669870&gt;, 'extract_text_from_pdf':\n            &lt;function extract_text_from_pdf at 0x7f6c2f6696c0&gt;,\n            'get_titles_and_first_authors': &lt;function\n            get_titles_and_first_authors at 0x7f6c2f66a170&gt;,\n            'summarize_local_document': &lt;function summarize_local_document\n            at 0x7f6c2f66ac20&gt;, 'describe_python_code': &lt;function\n            describe_python_code at 0x7f6c2f66b130&gt;, 'id_converter_tool':\n            &lt;function id_converter_tool at 0x7f6c2f66bb50&gt;,\n            'query_openalex_api': &lt;function query_openalex_api at\n            0x7f6c2f66bc70&gt;, 'query_semantic_scholar_api': &lt;function\n            query_semantic_scholar_api at 0x7f6c2f0c1480&gt;,\n            'respond_to_generic_queries': &lt;function\n            respond_to_generic_queries at 0x7f6c2f0c17e0&gt;},\n            add_tools:Dict[str,Any]={},\n            authentication:Optional[Dict[str,str]]=None,\n            dir_path:str='../data', messages:List[Dict[str,str]]=[])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstatus\ndict\n{}\nThe status of the assistant\n\n\nsys_message\nstr\nNone\nThe system message for the assistant; if not provided, a default message is used\n\n\nmodel\nstr\nllama3.1:latest\nThe model to use for the assistant\n\n\ntools\nDict\n{‘get_file_names’: &lt;function get_file_names at 0x7f6c2f669870&gt;, ‘extract_text_from_pdf’: &lt;function extract_text_from_pdf at 0x7f6c2f6696c0&gt;, ‘get_titles_and_first_authors’: &lt;function get_titles_and_first_authors at 0x7f6c2f66a170&gt;, ‘summarize_local_document’: &lt;function summarize_local_document at 0x7f6c2f66ac20&gt;, ‘describe_python_code’: &lt;function describe_python_code at 0x7f6c2f66b130&gt;, ‘id_converter_tool’: &lt;function id_converter_tool at 0x7f6c2f66bb50&gt;, ‘query_openalex_api’: &lt;function query_openalex_api at 0x7f6c2f66bc70&gt;, ‘query_semantic_scholar_api’: &lt;function query_semantic_scholar_api at 0x7f6c2f0c1480&gt;, ‘respond_to_generic_queries’: &lt;function respond_to_generic_queries at 0x7f6c2f0c17e0&gt;}\nThe tools available to the assistant\n\n\nadd_tools\nDict\n{}\nOptional argument to add additional tools to the assistant, when initializing\n\n\nauthentication\nOptional\nNone\nAuthentication credentials for API calls to external services\n\n\ndir_path\nstr\n../data\nThe directory path to which the assistant has access on the local computer\n\n\nmessages\nList\n[]\nThe conversation history\n\n\n\n\nsource\n\n\nAssistant.chat\n\n Assistant.chat (prompt:str, show_progress:bool=False,\n                 stream_response:bool=True, redirect_output:bool=False)\n\n*Start a conversation with the AI assistant.\nArgs: prompt (str): The user’s prompt or question. show_progress (bool): Whether to show the step-by-step progress of the fuction calls, including the tool calls and tool outputs. Default is False. stream_response (bool): Whether to stream the final response from the LLM. Default is True. Automatically set to True if redirect_output is True. redirect_output (bool): Whether to redirect the output to be compatible with st.write_stream. Default is False.\nReturns: str: The AI assistant’s response.*\n\nsource\n\n\nAssistant.show_conversation_history\n\n Assistant.show_conversation_history (show_function_calls:bool=False)\n\n*Display the conversation history.\nArgs: show_function_calls (bool): Whether to show function calls and returns in the conversation history. Default is False.\nReturns: None*\n\nsource\n\n\nAssistant.clear_conversation_history\n\n Assistant.clear_conversation_history ()\n\nClear the conversation history.\n\nsource\n\n\nAssistant.pprint_tools\n\n Assistant.pprint_tools ()\n\nPretty-print the descriptions of the available tools.\n\nsource\n\n\nAssistant.get_status\n\n Assistant.get_status ()\n\nGet the status of the assistant initialization.\n\nsource\n\n\nadd_to_class\n\n add_to_class (Class:type)\n\nRegister functions as methods in a class that has already been defined.",
    "crumbs": [
      "User & Developer Guide",
      "Source Code"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "User & Developer Guide",
    "section": "",
    "text": "Scholaris is a Python package that sets up a research assistant on your local computer, leveraging function calling capabilities. Designed specifically for health and life sciences, it helps researchers gain insights from scholarly articles by integrating with the Ollama Python library.",
    "crumbs": [
      "User & Developer Guide"
    ]
  },
  {
    "objectID": "index.html#about-scholaris",
    "href": "index.html#about-scholaris",
    "title": "User & Developer Guide",
    "section": "",
    "text": "Scholaris is a Python package that sets up a research assistant on your local computer, leveraging function calling capabilities. Designed specifically for health and life sciences, it helps researchers gain insights from scholarly articles by integrating with the Ollama Python library.",
    "crumbs": [
      "User & Developer Guide"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "User & Developer Guide",
    "section": "Key features",
    "text": "Key features\n\nLocal setup: No dependency on cloud-hosted LLMs for inference.\nExtract data from local files: Built-in tools to extract data from PDFs, py files, and plain text or markdown files.\nExternal data retrieval: Built-in tools to make API calls to external data sources, such as OpenAlex, Semantic Scholar, or NCBI’s Any ID converter.\nCustomizable and extensible architecture: Easily extend the functionality of the assistant by adding new tools with only a few lines of code.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis Python package is under active development and is not yet ready for production use. Report any issues or feature requests on the GitHub repository.",
    "crumbs": [
      "User & Developer Guide"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "User & Developer Guide",
    "section": "Installation",
    "text": "Installation\nStep 1. Download Ollama and follow the instructions to install Ollama for your operating system. Then, pull and run llama3.1 (parameters: 8B, quantization: Q4_0, size: 4.7 GB) according to the ollama documentation.\nStep 2. Go to your terminal and setup a new virtual environment, such as with Conda:\n\n\n\n\n\n\nConda Installation\n\n\n\n\n\nQuick command line instructions on how to install Miniconda, a free minimal installer for Conda, can be found here.\n\n\n\nconda create -n scholaris-env python=3.12\nStep 3. Activate the virtual environment:\nconda activate scholaris-env\nStep 4. Install latest scholaris Python package:\n\nfrom PyPIfrom the GitHub repository\n\n\npip install scholaris\n\n\npip install git+https://github.com/nicomarr/scholaris.git",
    "crumbs": [
      "User & Developer Guide"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "User & Developer Guide",
    "section": "Getting started",
    "text": "Getting started\nStep 1. Open a Jupyter notebook, IPython environment, or start Python from the terminal and import the scholaris core module:\n\nfrom scholaris.core import *\n\nStep 2. Initialize the Assistant class:\n\n\n\n\n\n\nPrerequisite: Ollama setup\n\n\n\n\n\nMake sure the Ollama app is installed on your local computer and Llama 3.1 8B has been downloaded and is running before initializing the assistant. Otherwise, the initialization will abort.\n\n\n\n\n\n\n\n\n\nDefault parameters\n\n\n\n\n\nIf no additional arguments are passed, the assistant is initialized with Llama 3.1 8B, a set of core functions and a default system message. During initialization, messages are printed to indicate whether credentials such as API keys and email are loaded from the environment variables (more on that below), and whether a local data directory already exists or has been created.\n\n\n\nInitialize the Assistant class with the default parameters:\n\nassistant = Assistant()\n\nLoaded Semantic Scholar API key from the environment variables.\nLoaded email address from the environment variables.\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\n\n\n\n\n\n\n\n\nSample file download\n\n\n\n\n\nFor the examples shown below, a local data directory had already been created in the parent directory prior to initialization, and a PDF file was downloaded and saved to the local data directory for demonstration purposes. You can download the same PDF file, or additional/other files by running the following commands in a Jupyter notebook cell. Alternatively, you can also download and add files manually to the local data directory after initialization.\n!mkdir -p ../data\npdf_urls = [\n    \"https://df6sxcketz7bb.cloudfront.net/manuscripts/144000/144499/jci.insight.144499.v2.pdf\",\n    # Add more URLs here as needed\n]\nfor url in pdf_urls:\n    !curl -o ../data/$(basename {url}) {url}\nTo see where data are stored afer initialization, simply call the dir_path attribute of the assistant object, like so:\nprint(assistant.dir_path)\n\n\n\nExplicitly set or change the model by passing the model name as an argument to the Assistant class:\n\nassistant = Assistant(model=\"llama3.1:latest\")\n\nLoaded Semantic Scholar API key from the environment variables.\nLoaded email address from the environment variables.\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\n\n\n\n\n\n\n\n\nSupported models\n\n\n\n\n\nDownload and select a model that supports tool calling. At the time of writing, the following models are supported:\n\nllama3.1\nllama3.2\nqwen2.5\nmistral-nemo\nnemotron-mini\ncommand-r\ncommand-r-plus\n\nFor more information, read the following blog post.",
    "crumbs": [
      "User & Developer Guide"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "User & Developer Guide",
    "section": "How to use",
    "text": "How to use\n\nChat\nTo chat with the assistant, simply call the chat() method with your prompt as input. You also can store the response in a variable, but this is optional. By default, the assistant will stream the response and store the conversation history.\n\nresponse = assistant.chat(\"Briefly tell me about the tools you have available.\")\n\nI can summarize research articles from various sources, provide information on scientific topics, and help with general knowledge queries. I have access to a range of tools that allow me to extract specific details from PDF documents, convert IDs between different formats (e.g., PubMed ID, DOI), and query OpenAlex and Semantic Scholar APIs for article metadata. Additionally, I can describe the purpose and content of local Python files. However, my capabilities are limited to the information provided by the available tools, so if a tool's content is empty or unavailable, I may not be able to provide a response.\n\n\n\n\n\n\n\n\nAccessing the messages\n\n\n\n\n\nYou can also access the assistant’s responses from the message attribute, like so:\nassistant.messages[-1][\"content\"] # Access the last message\n\n\n\nBy setting show_progress=True, you can see the step-by-step progress of the fuction calling. This includes the tool choice, selected arguments, if applicable, and the output of the called function that is being passed back to the LLM to generate the final response.\n\nresponse = assistant.chat(\"Which PDF files do you have access to in the local data directory\", show_progress=True)\n\nSelecting tools...\n\n[{'function': {'name': 'get_file_names', 'arguments': {'ext': 'pdf'}}}]\nCalling get_file_names() with arguments {'ext': 'pdf'}...\n\nGenerating final response...\nI have access to a single PDF file named \"jci.insight.144499.v2.pdf\" in the local data directory.\n\n\nBy default, streaming is enabled. If you like to disable streaming, set stream=False. This will store the entire conversation history in the messages attribute, which can be accessed as shown above.\n\nresponse = assistant.chat(\"Does this document have a title?\", stream_response=False)\n\nExtracting titles and first authors: 100%|██████████| 1/1 [00:07&lt;00:00,  7.72s/it]\n\n\n\n\nYes, the PDF file \"jci.insight.144499.v2.pdf\" has a title: \"Distinct antibody repertoires against endemic human coronaviruses in children and adults\".\n\n\n\n\nConversation history\nShow the conversation history by calling the assistant’s show_conversation_history() method:\n\nassistant.show_conversation_history()\n\nUser: Briefly tell me about the tools you have available.\n\nAssistant response: I can summarize research articles from various sources, provide information on scientific topics, and help with general knowledge queries. I have access to a range of tools that allow me to extract specific details from PDF documents, convert IDs between different formats (e.g., PubMed ID, DOI), and query OpenAlex and Semantic Scholar APIs for article metadata. Additionally, I can describe the purpose and content of local Python files. However, my capabilities are limited to the information provided by the available tools, so if a tool's content is empty or unavailable, I may not be able to provide a response.\n\nUser: Which PDF files do you have access to in the local data directory\n\nAssistant response: I have access to a single PDF file named \"jci.insight.144499.v2.pdf\" in the local data directory.\n\nUser: Does this document have a title?\n\nAssistant response: Yes, the PDF file \"jci.insight.144499.v2.pdf\" has a title: \"Distinct antibody repertoires against endemic human coronaviruses in children and adults\".\n\n\n\n\n\n\n\n\n\nShowing the conversation history with function calls\n\n\n\n\n\nThe show_conversation_history() method can be called with the show_function_calls=True argument to display the function calls made by the assistant during the conversation, and the output of the function calls. This can be useful for understanding the assistant’s responses, and for debugging purposes.\nassistant.show_conversation_history(show_function_calls=True)\n\n\n\nClear the conversation history by calling the assistant’s clear_conversation_history() method:\n\nassistant.clear_conversation_history()\n\n\n\nAssistant parameters\nShow the model by printing the assistant object:\n\nassistant\n\nAssistant, powered by llama3.1\n\n\nOr show the model by accessing the assistant’s model attribute:\n\nassistant.model\n\n'llama3.1:latest'\n\n\nShow the system messages by accessing the assistant’s sys_message attribute:\n\nprint(assistant.sys_message)\n\nYou are an AI assistant specialized in analyzing research articles.\n        Your role is to provide concise, human-readable responses based on information from tools and conversation history.\n\n        Key instructions:\n        1. Use provided tools to gather information before answering.\n        2. Interpret tool results and provide clear, concise answers in natural language.\n        3. If you can't answer with available tools, state this clearly.\n        4. Don't provide information if tool content is empty.\n        5. Never include raw JSON, tool outputs, or formatting tags in responses.\n        6. Format responses as plain text for direct human communication.\n        7. Use clear formatting (e.g., numbered or bulleted lists) when appropriate.\n        8. Provide article details (e.g., DOI, citation count) in a conversational manner.\n\n        Act as a knowledgeable research assistant, offering clear and helpful information based on available tools and data.\n        \n\n\n\n\nLocal data access\nBy default, the assistant has access to a single directory, called data. Within this directory, the assistant can list and read the following file formats and extensions: pdf, txt, md, markdown, csv, and py. If not already present, the directory is created when the assistant is initialized. If you want to change the directory name, you can do so by passing the desired directory name as an argument to the Assistant class when it is initialized. For example, to create a directory called proprietary_data, you would initialize the assistant as follows:\nassistant = Assistant(dir_path=\"../proprietary_data\")\n\n\nTools\nBy default, the assistant can call a set of core tools or functions which are passed to the Assistant as a dictionary when it is initialized. With these tools or functions, the assistant will be able to get a list of file names in a specific data directory, can extract content from these files, and summarize them. In addition, the assistant can make API calls to external data sources, such as OpenAlex or Semantic Scholar, to retrieve information about a large number of scholarly articles. The tools available to the assistant can be viewed by accessing the assistant’s list_tools() method as follows:\n\nassistant.list_tools()\n\nget_file_names\nextract_text_from_pdf\nget_titles_and_first_authors\nsummarize_local_document\ndescribe_python_code\nid_converter_tool\nquery_openalex_api\nquery_semantic_scholar_api\nrespond_to_generic_queries\ndescribe_tools\n\n\n\n\n\n\n\n\nDetailed tool information\n\n\n\n\n\nYou can learn more details about the core tools by visiting the Source Code page, which lists each function and provides a brief description of its purpose, functionality, required arguments, and usage (the docstring). This information helps you understand the available tools and how the LLM uses them. Alternatively, you can execute the assistant.pprint_tools() or assistant.get_tools_schema() methods.\n\n\n\n\n\nAuthentication for tool calling and API access (optional)\nSome tools take optional authentication parameters, such as an API key or email. For example, the query_semantic_scholar tool takes an optional API key to access the Semantic Scholar API, which will increase the API rate limit. Request a Semantic Scholar API Key here. Similarly, the query_openaplex_api tool takes an optional email parameter to access the OpenAlex API, which is recommended as a best practice and kindly requested by the API provider.\nThe best way to pass these parameters is to set them as environment variables, with the following key names: SEMANTIC_SCHOLAR_API_KEY and EMAIL. The Assistant class will automatically read these environment variables when initialized and pass them to the tools that require them. Alternatively, you can pass the Semantic Scholar API key and your email by simply adding the authentication argument when initializing the Assistant class, as shown below:\n\nauthentication = {\n    \"SEMANTIC_SCHOLAR_API_KEY\": \"your_api_key\",\n    \"EMAIL\": \"your_email@example.com\"\n}\nassistant = Assistant(authentication=authentication)\n\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\n\n\nIf you want to change the core functions, you can do so by passing the desired core functions as an argument to the Assistant class when it is initialized. For example, to limit the assitant’s ability to respond to generic questions and to access external data by making requests to the OpenAlex and Semantic Scholar API’s, you would initialize the assistant as follows:\n\nassistant = Assistant(tools = {\n    \"query_openalex_api\": query_openalex_api,\n    \"query_semantic_scholar_api\": query_semantic_scholar_api,\n    \"respond_to_generic_queries\": respond_to_generic_queries,\n    })\n\nLoaded Semantic Scholar API key from the environment variables.\nLoaded email address from the environment variables.\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\n\n\n\n\n\n\n\n\nWhat happens if the assistant is initialized without any tools?\n\n\n\n\n\nThe research assistent is set up so that it has to use a tool to generate a final response to a user’s prompt. This is to ensure that the assistant is primarily providing information which is relevant for health and life sciences. Otherwise it will abort the conversation, like so:\n\nassistant = Assistant(tools = {})\nassistant.chat(\"What is the capital of France?\")\n\nLoaded Semantic Scholar API key from the environment variables.\nLoaded email address from the environment variables.\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\nNo tools provided! Please add tools to the assistant.\nNo tool calls found in the response. Adding an empty tool_calls list to the conversation history. Aborting...\n\n\n\n\n\n\n\n\n\n\n\n\nFunction and method introspection\n\n\n\n\n\nWhen working in a Jupyter notebook or another iPython environment, you can quickly display details of a class method or fucntion by using special syntax. Type the name of the method or function, followed by a ?, or type ?? to get more detailed information (i.e., the docstring and basic information, or the source code, respectively). For example, to get information about the chat method, you can type the following:\nassistant.chat?\nassistant??",
    "crumbs": [
      "User & Developer Guide"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "User & Developer Guide",
    "section": "Developer Guide",
    "text": "Developer Guide\n\nDefining new tools\nYou can define new functions to be used by the assistant as tools. To simplify this process, a decorator called @json_schema_decorator is provided so it is not necessary to define the schema for the function. The schema is automatically generated based on the function’s annotation and docstring.\n\n\n\n\n\n\nKey points to consider\n\n\n\n\n\n\nUse type hints in the function signature to define the input and output types.\nUse Google style docstrings, as shown below, to describe the function’s purpose and the expected input and output.\nUse the @json_schema_decorator decorator to automatically generate the schema for the function.\nEnsure the output is a string (such as a JSON-formatted string) that can be passed back to the LLM to generate the final response.\n\n\n\n\n\n\n\n\n\n\nImportance of metadata for function calling\n\n\n\n\n\nIt’s crucial to understand that this metadata (function name, type hints, and docstring) is all the information the LLM has access to when deciding which function to call and how to use it. The LLM does not have access to or information about the actual source code or implementation of the functions (unless explicitly provided). Therefore, the metadata must be comprehensive and accurate to ensure proper function selection and usage by the LLM.\n\n\n\nThe following example shows how to define a new tool to multiply two numbers, which takes as input two integers or strings that can be converted to integers, and returns the product of the two numbers as a string:\n\nfrom typing import Union\n\n@json_schema_decorator\ndef multiply_two_numbers(a: Union[int, str], b: Union[int, str]) -&gt; str:\n    \"\"\"\n    A function to multiply two numbers.\n\n    Args:\n        a: First number, can be an integer or a string representation of an integer.\n        b: Second number, can be an integer or a string representation of an integer.\n\n    Returns:\n        str: The product of the two numbers, as a string.\n\n    Raises:\n        ValueError: If the inputs cannot be converted to integers.\n    \"\"\"\n    try:\n        int_a = int(a)\n        int_b = int(b)\n        return str(int_a * int_b)\n    except ValueError:\n        return \"Error: Inputs must be integers or string representations of integers.\"\n\n\n\n\n\n\n\nOptional arguments\n\n\n\n\n\nType hints are recommended but not required, unless you want to define optional arguments. In that case, you can use the Optional type hint from the typing module. This will determine which arguments are included in the required list values, as shown below.\n\n\n\nTo ensure the JSON schema is generated correctly, you can call the json_schema attribute of the function:\n\nmultiply_two_numbers.json_schema\n\n{'type': 'function',\n 'function': {'name': 'multiply_two_numbers',\n  'description': 'A function to multiply two numbers.',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'object',\n     'description': 'First number, can be an integer or a string representation of an integer.'},\n    'b': {'type': 'object',\n     'description': 'Second number, can be an integer or a string representation of an integer.'}},\n   'required': ['a', 'b']}}}\n\n\nAlternatively, you can use the generate_json_schema function:\n\ngenerate_json_schema(multiply_two_numbers)\n\n\n\nAdding tools\nYou can add new tools by passing a dictionary of new tools to the Assistant class when it is initialized. Use the add_tools argument to add new tools to the assistant. This will merge the new tools with the existing tools. For example, to add the new tool called multiply_two_integers to the assistant, you would initialize the assistant as follows:\n\nassistant = Assistant(add_tools = {\"multiply_two_numbers\": multiply_two_numbers})\n\nLoaded Semantic Scholar API key from the environment variables.\nLoaded email address from the environment variables.\nA local directory /Users/user2/GitHub/scholaris/data already exists for storing data files. No of files: 1\n\n\n\nYou can confirm that the new tool has been added to the list of existing tools by using the list_tools() method:\n\nassistant.list_tools()\n\nget_file_names\nextract_text_from_pdf\nget_titles_and_first_authors\nsummarize_local_document\ndescribe_python_code\nid_converter_tool\nquery_openalex_api\nquery_semantic_scholar_api\nrespond_to_generic_queries\ndescribe_tools\nmultiply_two_numbers\n\n\n\nresponse = assistant.chat(\"What is the product of 4173 and 351?\", show_progress=True)\n\nSelecting tools...\n\n[{'function': {'name': 'multiply_two_numbers', 'arguments': {'a': '4173', 'b': '351'}}}]\nCalling multiply_two_numbers() with arguments {'a': '4173', 'b': '351'}...\n\nGenerating final response...\nThe product of 4173 and 351 is 1,464,723.\n\n\n\n\nAdding methods\nYou can add new methods to the Assistant class by using the add_to_class() decorator function, like so:\n\n@add_to_class(Assistant)\ndef new_method(self):\n    # Method implementation\n    pass\n\n\n\nContributing to Scholaris\nThis Python package has been developed using nbdev. To contribute to this package, install nbdev and follow the nbdev documentation to set up your development environment.",
    "crumbs": [
      "User & Developer Guide"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "User & Developer Guide",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany thanks to the developers of Ollama and the Ollama Python Library for providing the core functionality that Scholaris is built upon, and thanks to all the providers of open-source and open-weight models. Special thanks to the developers of nbdev for making it easy to develop and document this package, and for many insightful tutorials and inspirations!",
    "crumbs": [
      "User & Developer Guide"
    ]
  }
]