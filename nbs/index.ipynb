{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scholaris\n",
    "\n",
    "> Scholaris is a Python library to set up a research assistant on your local computer, leveraging function calling capabilities. It is designed to help gain insights from scholarly articles in health and life sciences, and is build on top of the [Ollama Python library](https://pypi.org/project/ollama/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "This library is under active development and is not yet ready for production use. Report any issues or feature requests on the [GitHub repository](https://github.com/nicomarr/scholaris/issues).\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.** [Download Ollama](https://ollama.com/download) and follow the instructions to install Ollama for your operating system. Then, pull and run [llama3.1](https://ollama.com/library/llama3.1) (parameters: 8B, quantization: Q4_0, size: 4.7 GB) according to the ollama documentation.\n",
    "\n",
    "**Step 2.** Setup a new virtual environment, such as with Conda. Quick command line instructions on how to install  Miniconda, a free minimal installer for Conda, can be found [here](https://docs.anaconda.com/free/miniconda/#quick-command-line-install).\n",
    "\n",
    "\n",
    "```sh\n",
    "$ conda create -n scholaris-env python=3.12\n",
    "```\n",
    "\n",
    "**Step 3.** Activate the virtual environment:\n",
    "\n",
    "```sh\n",
    "$ conda activate scholaris-env\n",
    "```\n",
    "\n",
    "**Step 4.** Install latest scholaris Python package from the GitHub [repository][repo]:\n",
    "\n",
    "```sh\n",
    "$ pip install git+https://github.com/nicomarr/scholaris.git\n",
    "```\n",
    "\n",
    "\n",
    "or from [pypi][pypi]:\n",
    "\n",
    "\n",
    "```sh\n",
    "$ pip install scholaris\n",
    "```\n",
    "\n",
    "\n",
    "[repo]: https://github.com/nicomarr/scholaris\n",
    "[pypi]: https://pypi.org/project/scholaris/\n",
    "\n",
    "**Step 5.** Install the following dependencies:\n",
    "\n",
    "```sh\n",
    "$ pip install ollama\n",
    "$ pip install PyPDF2\n",
    "$ pip install requests\n",
    "$ pip install tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the scholaris core module. If you work from a Jupyter notebook environment within a different directory, you may need to add the parent directory of the current working directory to the Python system path as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # To add the parent directory to the path\n",
    "from scholaris.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the Ollama app is installed on your local computer and Llama 3.1 8B has been downloaded and is running. Then, initialize the `Assistant` class with Llama 3.1 8B, the core functions and default system message. During initialization, messages are printed to indicate whether credentials such as API keys and email are loaded from the environment variables (more on that below), and whether a local data directory already exists or has been created, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Semantic Scholar API key from the environment variables.\n",
      "Loaded email address from the environment variables.\n",
      "A local directory ../data already exists for storing data files. No of files: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = Assistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "For the examples shown below, a local data directory had already been created in the parent directory prior to initialization, and a PDF file was downloaded and saved to the local data directory for demonstration purposes. You can download the same PDF file, or additional/other files by running the following commands in a Jupyter notebook cell. Alternatively, you can download and add the files manually to the local data directory if the assistant has already been initialized.\n",
    "\n",
    "    \n",
    "```python\n",
    "!mkdir -p ../data\n",
    "pdf_urls = [\n",
    "    \"https://df6sxcketz7bb.cloudfront.net/manuscripts/144000/144499/jci.insight.144499.v2.pdf\",\n",
    "    # Add more URLs here as needed\n",
    "]\n",
    "for url in pdf_urls:\n",
    "    !curl -o ../data/$(basename {url}) {url}\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly set the model by passing the model name as an argument to the `Assistant` class. If you do not pass a model name, the default model is ***Llama 3.1*** 8B. An alternative model that supports tool calling and can be run on a standard laptop is Mistral's ***NeMo*** 12 B model. To use the latter model, change the attribute in the `Assistant` class to \"mistral-nemo\". For more information, read the following [blog post](https://ollama.com/blog/tool-support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Semantic Scholar API key from the environment variables.\n",
      "Loaded email address from the environment variables.\n",
      "A local directory ../data already exists for storing data files. No of files: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = Assistant(model=\"llama3.1:latest\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To chat with the assistant, simply call the `chat()` method with your prompt as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[94mI\u001b[0m\u001b[94m have\u001b[0m\u001b[94m access\u001b[0m\u001b[94m to\u001b[0m\u001b[94m \u001b[0m\u001b[94m1\u001b[0m\u001b[94m PDF\u001b[0m\u001b[94m file\u001b[0m\u001b[94m on\u001b[0m\u001b[94m the\u001b[0m\u001b[94m local\u001b[0m\u001b[94m computer\u001b[0m\u001b[94m:\n",
      "\n",
      "\u001b[0m\u001b[94m*\u001b[0m\u001b[94m j\u001b[0m\u001b[94mci\u001b[0m\u001b[94m.ins\u001b[0m\u001b[94might\u001b[0m\u001b[94m.\u001b[0m\u001b[94m144\u001b[0m\u001b[94m499\u001b[0m\u001b[94m.v\u001b[0m\u001b[94m2\u001b[0m\u001b[94m.pdf\u001b[0m\u001b[94m\u001b[0m"
     ]
    }
   ],
   "source": [
    "assistant.chat(\"Which pdf files do you have access to on the local computer?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `show_progress=True`, you can see the step-by-step progress of the fuction calling. This includes the tool choice, selected arguments, if applicable, and the output of the called function that is being passed back to the LLM to generate the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tools...\n",
      "\n",
      "[{'function': {'name': 'get_titles_and_first_authors', 'arguments': {}}}]\n",
      "Calling get_titles_and_first_authors() with arguments {}...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting titles and first authors: 100%|██████████| 1/1 [00:07<00:00,  7.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Function response:\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Distinct antibody repertoires against endemic human coronaviruses in children and adults\",\n",
      "    \"first_author\": \"Taushif Khan\",\n",
      "    \"file_name\": \"jci.insight.144499.v2.pdf\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Generating final response...\n",
      "\n",
      "\n",
      "\u001b[94mThe\u001b[0m\u001b[94m title\u001b[0m\u001b[94m of\u001b[0m\u001b[94m the\u001b[0m\u001b[94m PDF\u001b[0m\u001b[94m file\u001b[0m\u001b[94m is\u001b[0m\u001b[94m:\n",
      "\n",
      "\u001b[0m\u001b[94m\"\u001b[0m\u001b[94mDistinct\u001b[0m\u001b[94m antibody\u001b[0m\u001b[94m rep\u001b[0m\u001b[94merto\u001b[0m\u001b[94mires\u001b[0m\u001b[94m against\u001b[0m\u001b[94m endemic\u001b[0m\u001b[94m human\u001b[0m\u001b[94m coron\u001b[0m\u001b[94mavir\u001b[0m\u001b[94muses\u001b[0m\u001b[94m in\u001b[0m\u001b[94m children\u001b[0m\u001b[94m and\u001b[0m\u001b[94m adults\u001b[0m\u001b[94m\"\u001b[0m\u001b[94m\u001b[0m"
     ]
    }
   ],
   "source": [
    "assistant.chat(\"Get the title of this PDF file.\", show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get information about a research article from external sources, simply ask the assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[94mThe\u001b[0m\u001b[94m study\u001b[0m\u001b[94m entitled\u001b[0m\u001b[94m '\u001b[0m\u001b[94mT\u001b[0m\u001b[94mub\u001b[0m\u001b[94merculosis\u001b[0m\u001b[94m and\u001b[0m\u001b[94m impaired\u001b[0m\u001b[94m IL\u001b[0m\u001b[94m-\u001b[0m\u001b[94m23\u001b[0m\u001b[94m–\u001b[0m\u001b[94mdependent\u001b[0m\u001b[94m IF\u001b[0m\u001b[94mN\u001b[0m\u001b[94m-\u001b[0m\u001b[94mγ\u001b[0m\u001b[94m immunity\u001b[0m\u001b[94m in\u001b[0m\u001b[94m humans\u001b[0m\u001b[94m homo\u001b[0m\u001b[94mzy\u001b[0m\u001b[94mg\u001b[0m\u001b[94mous\u001b[0m\u001b[94m for\u001b[0m\u001b[94m a\u001b[0m\u001b[94m common\u001b[0m\u001b[94m TY\u001b[0m\u001b[94mK\u001b[0m\u001b[94m2\u001b[0m\u001b[94m miss\u001b[0m\u001b[94mense\u001b[0m\u001b[94m variant\u001b[0m\u001b[94m'\u001b[0m\u001b[94m has\u001b[0m\u001b[94m been\u001b[0m\u001b[94m cited\u001b[0m\u001b[94m \u001b[0m\u001b[94m161\u001b[0m\u001b[94m times\u001b[0m\u001b[94m.\u001b[0m\u001b[94m\u001b[0m"
     ]
    }
   ],
   "source": [
    "assistant.chat(\"Get the number of citations for the study entitled 'Tuberculosis and impaired IL-23–dependent IFN-γ immunity in humans homozygous for a common TYK2 missense variant.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the conversation history by calling the assistant's `show_conversation_history()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mUser:\u001b[0m Which pdf files do you have access to on the local computer?\n",
      "\n",
      "\u001b[1m\u001b[94mAssistant function calls:\u001b[0m \u001b[94mget_file_names() with arguments {'ext': 'pdf'}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[90mFunction return:\u001b[0m \u001b[90mList of file names with the specified extensions in the local data directory: jci.insight.144499.v2.pdf\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[94mAssistant response:\u001b[0m \u001b[94mI have access to 1 PDF file on the local computer:\n",
      "\n",
      "* jci.insight.144499.v2.pdf\u001b[0m\n",
      "\n",
      "\u001b[1mUser:\u001b[0m Get the title of this PDF file.\n",
      "\n",
      "\u001b[1m\u001b[94mAssistant function calls:\u001b[0m \u001b[94mget_titles_and_first_authors() with arguments {}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[90mFunction return:\u001b[0m \u001b[90m[\n",
      "  {\n",
      "    \"title\": \"Distinct antibody repertoires against endemic human coronaviruses in children and adults\",\n",
      "    \"first_author\": \"Taushif Khan\",\n",
      "    \"file_name\": \"jci.insight.144499.v2.pdf\"\n",
      "  }\n",
      "]\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[94mAssistant response:\u001b[0m \u001b[94mThe title of the PDF file is:\n",
      "\n",
      "\"Distinct antibody repertoires against endemic human coronaviruses in children and adults\"\u001b[0m\n",
      "\n",
      "\u001b[1mUser:\u001b[0m Get the number of citations for the study entitled 'Tuberculosis and impaired IL-23–dependent IFN-γ immunity in humans homozygous for a common TYK2 missense variant.'\n",
      "\n",
      "\u001b[1m\u001b[94mAssistant function calls:\u001b[0m \u001b[94mquery_openalex_api() with arguments {'identifyer': 'Tuberculosis and impaired IL-23–dependent IFN-γ immunity in humans homozygous for a common TYK2 missense variant.', 'identifyer_type': 'title'}\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[90mFunction return:\u001b[0m \u001b[90m[\n",
      "  {\n",
      "    \"id\": \"https://openalex.org/W2906653622\",\n",
      "    \"doi\": \"https://doi.org/10.1126/sciimmunol.aau8714\",\n",
      "    \"title\": \"Tuberculosis and impaired IL-23\\u2013dependent IFN-\\u03b3 immunity in humans homozygous for a common <i>TYK2</i> missense variant\",\n",
      "    \"publication_year\": 2018,\n",
      "    \"cited_by_count\": 161,\n",
      "    \"cited_by_api_url\": \"https://api.openalex.org/works?filter=cites:W2906653622\",\n",
      "    \"open_access\": {\n",
      "      \"is_oa\": true,\n",
      "      \"oa_status\": \"bronze\",\n",
      "      \"oa_url\": \"https://immunology.sciencemag.org/content/immunology/3/30/eaau8714.full.pdf\",\n",
      "      \"any_repository_has_fulltext\": true\n",
      "    },\n",
      "    \"type\": \"article\",\n",
      "    \"type_crossref\": \"journal-article\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"https://openalex.org/W4249745515\",\n",
      "    \"doi\": \"https://doi.org/10.3410/f.734676937.793572033\",\n",
      "    \"title\": \"Faculty Opinions recommendation of Tuberculosis and impaired IL-23-dependent IFN-\\u03b3 immunity in humans homozygous for a common TYK2 missense variant.\",\n",
      "    \"publication_year\": 2020,\n",
      "    \"cited_by_count\": 0,\n",
      "    \"cited_by_api_url\": \"https://api.openalex.org/works?filter=cites:W4249745515\",\n",
      "    \"open_access\": {\n",
      "      \"is_oa\": true,\n",
      "      \"oa_status\": \"bronze\",\n",
      "      \"oa_url\": \"https://facultyopinions.com/download/734676937\",\n",
      "      \"any_repository_has_fulltext\": true\n",
      "    },\n",
      "    \"type\": \"dataset\",\n",
      "    \"type_crossref\": \"dataset\"\n",
      "  }\n",
      "]\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[94mAssistant response:\u001b[0m \u001b[94mThe study entitled 'Tuberculosis and impaired IL-23–dependent IFN-γ immunity in humans homozygous for a common TYK2 missense variant' has been cited 161 times.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant.show_conversion_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear the conversation history by calling the assistant's `clear_conversation_history()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.clear_conversion_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistant parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the model by printing the assistant object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assistant, powered by llama3.1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or show the model by accessing the assistant's model attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama3.1:latest'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the system messages by accessing the assistant's sys_message attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specialized in analyzing research articles.\n",
      "        Your role is to provide concise, human-readable responses based on information from tools and conversation history.\n",
      "\n",
      "        Key instructions:\n",
      "        1. Use provided tools to gather information before answering.\n",
      "        2. Interpret tool results and provide clear, concise answers in natural language.\n",
      "        3. If you can't answer with available tools, state this clearly.\n",
      "        4. Don't provide information if tool content is empty.\n",
      "        5. Never include raw JSON, tool outputs, or formatting tags in responses.\n",
      "        6. Format responses as plain text for direct human communication.\n",
      "        7. Use clear formatting (e.g., numbered or bulleted lists) when appropriate.\n",
      "        8. Provide article details (e.g., DOI, citation count) in a conversational manner.\n",
      "\n",
      "        Act as a knowledgeable research assistant, offering clear and helpful information based on available tools and data.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(assistant.sys_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local data access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the assistant has access to a single directory, called `data`. Within this directory, the assistant can list and read the following file formats and extensions: pdf, txt, md, markdown, csv, and py.\n",
    "If not already present, the directory is created when the assistant is initialized. If you want to change the directory name, you can do so by passing the desired directory name as an argument to the `Assistant` class when it is initialized. For example, to create a directory called `proprietary_data`, you would initialize the assistant as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Semantic Scholar API key from the environment variables.\n",
      "Loaded email address from the environment variables.\n",
      "Created directory ../proprietary_data for storing data files.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = Assistant(dir_path=\"../proprietary_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the assistant can call a set of core tools or functions which are passed to the `Assistant` as a dictionary when it is initialized. With these tools or functions, the assistant will be able to get a list of file names in a specific data directory, can extract content from these files, and summarize them. In addition, the assistant can make API calls to external data sources, such as [OpenAlex](https://openalex.org) or [Semantic Scholar](https://www.semanticscholar.org), to retrieve information about a large number of scholarly articles. The tools available to the assistant can be viewed by accessing the assistant's `list_tools()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_file_names\n",
      "extract_text_from_pdf\n",
      "get_titles_and_first_authors\n",
      "summarize_local_document\n",
      "describe_python_code\n",
      "query_openalex_api\n",
      "query_semantic_scholar_api\n",
      "respond_to_generic_queries\n"
     ]
    }
   ],
   "source": [
    "assistant.list_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "You can learn more details about the core tools by visiting the Source Code page, which lists each function and provides a brief description of its purpose, functionality, required arguments, and usage (the docstring). This information helps you understand the available tools and how the LLM uses them. Alternatively, if you prefer to see the schema for all tools the assistant can call, you can execute the `assistant.get_tools_schema()` method.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication for tool calling and API access (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tools take optional authentication parameters, such as an API key or email. For example, the `query_semantic_scholar` tool takes an optional API key to access the Semantic Scholar API, which will increase the API rate limit. Request a Semantic Scholar API Key [here](https://www.semanticscholar.org/product/api/tutorial). Similarly, the `query_openaplex_api` tool takes an optional email parameter to access the OpenAlex API, which is recommended as a best practice and kindly requested by the [API provider](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool). \n",
    "\n",
    "The best way to pass these parameters is to set them as environment variables, with the following key names: `SEMANTIC_SCHOLAR_API_KEY` and `EMAIL`. The Assistant class will automatically read these environment variables when initialized and pass them to the tools that require them. Alternatively, you can pass the Semantic Scholar API key and your email by simply adding the authentication argument when initializing the Assistant class, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local directory ../data already exists for storing data files. No of files: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authentication = {\n",
    "    \"SEMANTIC_SCHOLAR_API_KEY\": \"your_api_key\",\n",
    "    \"EMAIL\": \"your_email@example.com\"\n",
    "}\n",
    "assistant = Assistant(authentication=authentication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you want to change the core functions, you can do so by passing the desired core functions as an argument to the Assistant class when it is initialized. For example, to limit the assitant's ability to respond to generic questions and to access external data by making requests to the OpenAlex and Semantic Scholar API's, you would initialize the assistant as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Semantic Scholar API key from the environment variables.\n",
      "Loaded email address from the environment variables.\n",
      "A local directory ../data already exists for storing data files. No of files: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = Assistant(tools = {\n",
    "    \"query_openalex_api\": query_openalex_api,\n",
    "    \"query_semantic_scholar_api\": query_semantic_scholar_api,\n",
    "    \"respond_to_generic_queries\": respond_to_generic_queries,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "The research assistent is set up so that it has to use a tool to generate a final response to a user's prompt. This is to ensure that the assistant is primarily providing information which is relevant for health and life sciences. Otherwise it will abort the conversation, like so:\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Semantic Scholar API key from the environment variables.\n",
      "Loaded email address from the environment variables.\n",
      "A local directory ../data already exists for storing data files. No of files: 1\n",
      "\n",
      "\u001b[91mNo tools provided! Please add tools to the assistant.\u001b[0m\n",
      "\u001b[91mNo tool calls found in the response. Adding an empty tool_calls list to the conversation history. Aborting...\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = Assistant(tools = {})\n",
    "assistant.chat(\"What is the capital of France?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define new functions to be used by the assistant as tools. To simplify this process, a decorator called `@json_schema_decorator` is provided so it is not necessary to define the schema for the function. The schema is automatically generated based on the function's annotation and docstring. \n",
    "\n",
    "::: {.callout-tip}\n",
    "Here are key points to consider when defining a new tool:\n",
    "\n",
    "- Use type hints in the function signature to define the input and output types.\n",
    "- Use docstrings, as shown below, to describe the function's purpose and the expected input and output.\n",
    "- Use the `@json_schema_decorator` decorator to automatically generate the schema for the function.\n",
    "- Ensure the output is a string (such as a JSON-formatted string) that can be passed back to the LLM to generate the final response.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "It's crucial to understand that this metadata (function name, type hints, and docstring) is all the information the LLM has access to when deciding which function to call and how to use it. The LLM does not have access to or information about the actual source code or implementation of the functions (unless explicitly provided). Therefore, the metadata must be comprehensive and accurate to ensure proper function selection and usage by the LLM. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how to define a new tool called `multiply_two_integers` that takes two integers as input and returns a string as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@json_schema_decorator\n",
    "def multiply_two_integers(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    A function to multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a (int): First integer.\n",
    "        b (int): Second integer.\n",
    "\n",
    "    Returns:\n",
    "        str: The product of the two numbers, as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(a, int) or not isinstance(b, int):\n",
    "        return \"Error: Inputs must be integers.\"\n",
    "    \n",
    "    return str(a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the JSON schema is generated correctly, you can call the `json_schema` attribute of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'multiply_two_integers',\n",
       "  'description': 'A function to multiply two numbers.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'a': {'type': 'integer', 'description': 'First integer.'},\n",
       "    'b': {'type': 'integer', 'description': 'Second integer.'}},\n",
       "   'required': ['a', 'b']}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply_two_integers.json_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the `generate_json_schema` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'multiply_two_integers',\n",
       "  'description': 'A function to multiply two numbers.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'a': {'type': 'integer', 'description': 'First integer.'},\n",
       "    'b': {'type': 'integer', 'description': 'Second integer.'}},\n",
       "   'required': ['a', 'b']}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_json_schema(multiply_two_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add new tools by passing a dictionary of new tools to the `Assistant` class when it is initialized. Use the `add_tools` argument to add new tools to the assistant. This will merge the new tools with the existing tools. For example, to add the new tool called `multiply_two_integers` to the assistant, you would initialize the assistant as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Semantic Scholar API key from the environment variables.\n",
      "Loaded email address from the environment variables.\n",
      "A local directory ../data already exists for storing data files. No of files: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = Assistant(add_tools = {\"multiply_two_integers\": multiply_two_integers})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm that the new tool has been added to the list of existing tools by using the `list_tools()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_file_names\n",
      "extract_text_from_pdf\n",
      "get_titles_and_first_authors\n",
      "summarize_local_document\n",
      "describe_python_code\n",
      "query_openalex_api\n",
      "query_semantic_scholar_api\n",
      "respond_to_generic_queries\n",
      "multiply_two_integers\n"
     ]
    }
   ],
   "source": [
    "assistant.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tools...\n",
      "\n",
      "[{'function': {'name': 'multiply_two_integers', 'arguments': {'a': 4173, 'b': 351}}}]\n",
      "Calling multiply_two_integers() with arguments {'a': 4173, 'b': 351}...\n",
      "\n",
      "Function response:\n",
      "1464723\n",
      "\n",
      "Generating final response...\n",
      "\n",
      "\n",
      "\u001b[94mThe\u001b[0m\u001b[94m product\u001b[0m\u001b[94m of\u001b[0m\u001b[94m \u001b[0m\u001b[94m417\u001b[0m\u001b[94m3\u001b[0m\u001b[94m and\u001b[0m\u001b[94m \u001b[0m\u001b[94m351\u001b[0m\u001b[94m is\u001b[0m\u001b[94m \u001b[0m\u001b[94m1\u001b[0m\u001b[94m,\u001b[0m\u001b[94m464\u001b[0m\u001b[94m,\u001b[0m\u001b[94m723\u001b[0m\u001b[94m.\u001b[0m\u001b[94m\u001b[0m"
     ]
    }
   ],
   "source": [
    "assistant.chat(\"What is the product of 4173 and 351?\", show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding methods\n",
    "\n",
    "You can add new methods to the `Assistant` class by using the `add_to_class()` decorator function. For example, the `clear_conversion_history` method was added to the `Assistant` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(Assistant)\n",
    "def clear_conversion_history(self):\n",
    "    \"\"\"Clear the conversation history.\"\"\"\n",
    "    self.messages = [{'role': \"system\", 'content': self.sys_message},]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
